---
abstract: We study methods for assessing the degree of systematic over- or under-
  estimation, known as calibration, of a learned risk model in an independent validation
  cohort. Here, we advance methods for evaluating clinical risk prediction models
  by deriving a population parameter measuring the average calibration error of the
  predicted risk from the true risk, and providing a method for estimation and inference.
  Our approach improves upon commonly-used goodness of t tests that depends on subjective
  bin thresholding and may yield misleading results by reporting confidence intervals
  for the calibration error instead of a simple P-value that conflate calibration
  error and sample size. This approach enables comparison among multiple risk prediction
  models, and can guide model revision. We illustrate how our new method helps to
  understand the calibration of risk models that have been profoundly influential
  in clinical practice, but controversial due to their potential miscalibration.
booktitle: Proceedings of the 4th Machine Learning for Healthcare Conference
title: A Calibration Metric for Risk Scores with Survival Data
volume: '106'
year: '2019'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yadlowsky19a
month: 0
tex_title: A Calibration Metric for Risk Scores with Survival Data
firstpage: 424
lastpage: 450
page: 424-450
order: 424
cycles: false
bibtex_author: Yadlowsky, Steve and Basu, Sanjay and Tian, Lu
author:
- given: Steve
  family: Yadlowsky
- given: Sanjay
  family: Basu
- given: Lu
  family: Tian
date: 2019-10-28
address: 
publisher: PMLR
container-title: Proceedings of the 4th Machine Learning for Healthcare Conference
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 28
pdf: http://proceedings.mlr.press/v106/yadlowsky19a/yadlowsky19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
